{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTのファインチューニング入門"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch] ipywidgets datasets accelerate evaluate matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードを42に固定\n",
    "set_seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語モデルを使ってプログラミング言語の判定を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットを準備する\n",
    "\n",
    "datasetという、huggingfaceが開発したライブラリを使ってデータの読み込みを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/code-search-net/code_search_net\n",
    "\n",
    "dataset = load_dataset(\"code-search-net/code_search_net\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分布を見る\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_dataset(dataset):\n",
    "\n",
    "    # 言語名の出現回数を数える\n",
    "    try:\n",
    "        lang_counts = Counter(dataset[\"train\"][\"language\"])\n",
    "    except:\n",
    "        lang_counts = Counter(dataset[\"language\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 件数の多い順に並べ替え\n",
    "    labels, values = zip(*sorted(lang_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # グラフを描画\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.title(\"Distribution of samples by language (>100 samples only)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 今回扱うデータは分布が不均衡なデータセット。\n",
    "- 学習が終わるようにそれぞれの言語のデータセット数を制限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# 元の DatasetDict を使って分割取得\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# DataFrame に変換\n",
    "df_train = train_dataset.to_pandas()\n",
    "df_test = test_dataset.to_pandas()\n",
    "df_val = val_dataset.to_pandas()\n",
    "\n",
    "def sample_per_language(df, is_validation=False):\n",
    "    return df.groupby(\"language\").apply(\n",
    "        lambda x: x.sample(\n",
    "            n=min(100, len(x)) if not is_validation else min(500, len(x)),\n",
    "            random_state=42\n",
    "        )\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "\n",
    "sampled_df_train = sample_per_language(df_train)\n",
    "sampled_df_test = sample_per_language(df_test)\n",
    "sampled_df_val = sample_per_language(df_val,is_validation=True)\n",
    "\n",
    "# Dataset に戻す（index列の削除も忘れずに）\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(sampled_df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(sampled_df_test, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(sampled_df_val, preserve_index=False),\n",
    "})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数のトークナイズ処理を定義\n",
    "def tokenize_function(example):\n",
    "    tokenized_example = tokenizer(example[\"whole_func_string\"], max_length=512)\n",
    "    # ラベルとして言語ID（数値）を追加\n",
    "    tokenized_example[\"labels\"] = example[\"language\"]\n",
    "    return tokenized_example\n",
    "\n",
    "# 事前学習済みBERTモデルのトークナイザーを読み込み\n",
    "model_name = \"google-bert/bert-large-cased\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)    \n",
    "\n",
    "# 元の言語ラベル（文字列）を保存する列を追加（後で逆変換のために利用できる）\n",
    "dataset = dataset.map(lambda x: {\"language_str\": x[\"language\"]})\n",
    "\n",
    "# 言語列をカテゴリとしてエンコード（数値に変換）\n",
    "dataset = dataset.class_encode_column(\"language\")\n",
    "\n",
    "# 各データをトークナイズし、ラベル情報を付加\n",
    "tokenize_dataset = dataset.map(tokenize_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込みと設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"]\n",
    "#　目的変数をlanguageとして分類タスクを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-bert/bert-large-cased https://huggingface.co/docs/transformers/main/en/model_doc/auto#natural-language-processing\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           num_labels=len(set(dataset[\"train\"]['language'])),\n",
    "                                                           device_map=\"cuda:0\"\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # (predictions == labels)        → [True, False, True]\n",
    "    # (predictions == labels).mean() → (1 + 0 + 1) / 3 \n",
    "\n",
    "\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習に関する設定をまとめるオブジェクト\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-classification-language\",  # 学習済みモデルの保存先ディレクトリ\n",
    "    learning_rate=2e-5,                         # 学習率（小さすぎても大きすぎても学習が不安定になるため適切に設定）\n",
    "    per_device_train_batch_size=16,            # 1デバイスあたりの学習時バッチサイズ\n",
    "    per_device_eval_batch_size=16,             # 1デバイスあたりの評価時バッチサイズ\n",
    "    eval_strategy=\"epoch\",                     # 各エポック終了時に評価を実施\n",
    "    logging_strategy=\"epoch\",                  # 各エポック終了時にログを記録\n",
    "    num_train_epochs=3,                        # 学習のエポック数\n",
    "    weight_decay=0.01,                         # 過重みの減衰\n",
    "    report_to=\"none\"                           # ロギングツール（例: TensorBoard）への出力を行わない\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# トークナイズされた入力データに対して、バッチごとに自動でパディングを行う\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenize_dataset[\"train\"],\n",
    "    eval_dataset=tokenize_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"validation\"][1]\n",
    "inputs = tokenizer(sample[\"whole_func_string\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "pred   = model(**inputs).logits.argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sample = dataset[\"validation\"][1]\n",
    "inputs = tokenizer(sample[\"whole_func_string\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    pred = model(**inputs).logits.argmax(-1).item()\n",
    "\n",
    "is_correct = pred == sample[\"language\"]\n",
    "print(f\"予測ラベル: {pred}, 正解ラベル: {sample['language']}, 一致しているか: {is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解数のカウント\n",
    "correct = 0\n",
    "\n",
    "# 検証データのサンプル数を取得\n",
    "total = len(dataset[\"validation\"])\n",
    "\n",
    "# 検証データの全サンプルに対してループ\n",
    "for i in range(total):\n",
    "    sample = dataset[\"validation\"][i]\n",
    "\n",
    "    # 入力テキストをトークナイズ\n",
    "    inputs = tokenizer(\n",
    "        sample[\"whole_func_string\"],\n",
    "        return_tensors=\"pt\",       # PyTorch形式のテンソルに変換\n",
    "        truncation=True,           # 最大長を超える部分は切り捨て\n",
    "        padding=True               # 短い文はパディングで調整\n",
    "    ).to(\"cuda\")                   # GPU上で処理\n",
    "\n",
    "    # 推論モード\n",
    "    with torch.no_grad():\n",
    "        # モデルに入力して予測ラベルを取得\n",
    "        pred = model(**inputs).logits.argmax(-1).item()\n",
    "\n",
    "    # 予測が正解ラベルと一致していればカウントを増やす\n",
    "    if pred == sample[\"language\"]:\n",
    "        correct += 1\n",
    "\n",
    "# 全体に対する正解率を計算\n",
    "accuracy = correct / total\n",
    "\n",
    "# 結果を出力\n",
    "print(f\"正解数: {correct}/{total}, 正解率: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"./language-bert-model\"\n",
    "trainer.save_model(SAVE_DIR)                # モデルの保存\n",
    "tokenizer.save_pretrained(SAVE_DIR)              # トークナイザも忘れずに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmZemi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
