{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語モデルのファインチューニング入門"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate evaluate matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードを42に固定\n",
    "set_seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語モデルを使ってプログラミング言語の判定を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットを準備する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"code-search-net/code_search_net\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分布を見る\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_dataset(dataset):\n",
    "\n",
    "    # 言語名の出現回数を数える\n",
    "    try:\n",
    "        lang_counts = Counter(dataset[\"train\"][\"language\"])\n",
    "    except:\n",
    "        lang_counts = Counter(dataset[\"language\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 件数の多い順に並べ替え\n",
    "    labels, values = zip(*sorted(lang_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # グラフを描画\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.title(\"Distribution of samples by language (>100 samples only)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 今回扱うデータは分布が不均衡なデータセット。\n",
    "- 学習が終わるようにそれぞれの言語のデータセット数を制限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# 元の DatasetDict を使って分割取得\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# DataFrame に変換\n",
    "df_train = train_dataset.to_pandas()\n",
    "df_test = test_dataset.to_pandas()\n",
    "df_val = val_dataset.to_pandas()\n",
    "\n",
    "def sample_per_language(df, is_validation=False):\n",
    "    return df.groupby(\"language\").apply(\n",
    "        lambda x: x.sample(\n",
    "            n=min(100, len(x)) if not is_validation else min(500, len(x)),\n",
    "            random_state=42\n",
    "        )\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "\n",
    "sampled_df_train = sample_per_language(df_train)\n",
    "sampled_df_test = sample_per_language(df_test)\n",
    "sampled_df_val = sample_per_language(df_val,is_validation=True)\n",
    "\n",
    "# Dataset に戻す（index列の削除も忘れずに）\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(sampled_df_train, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(sampled_df_test, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(sampled_df_val, preserve_index=False),\n",
    "})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example):\n",
    "    tokenized_example= tokenizer(example[\"whole_func_string\"], max_length=512)\n",
    "    tokenized_example[\"labels\"] = example[\"language\"]\n",
    "    return tokenized_example\n",
    "\n",
    "model_name = \"google-bert/bert-large-cased\"\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)    \n",
    "\n",
    "languages = dataset[\"train\"].features[\"language\"].names\n",
    "\n",
    "\n",
    "dataset=dataset.class_encode_column(\"language\")\n",
    "\n",
    "\n",
    "tokenize_dataset = dataset.map(tokenize_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込みと設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"]\n",
    "#　目的変数をlanguage_nameとして分類タスクを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           num_labels=len(set(dataset[\"train\"]['language'])),\n",
    "                                                           device_map=\"auto\"\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # predictionsは各ラベルについてのスコア\n",
    "    # 最もスコアの高いインデックスを予測ラベルとする\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1) \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir          = \"bert-classification-language\",\n",
    "    learning_rate       = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 16,\n",
    "    num_train_epochs    = 3,\n",
    "    weight_decay        = 0.01,\n",
    "    report_to           = \"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenize_dataset[\"train\"],\n",
    "    eval_dataset=tokenize_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"validation\"][1][\"whole_func_string\"]\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(\"cuda\")\n",
    "pred   = model(**inputs).logits.argmax(-1).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",      # モデルが分類タスクの場合\n",
    "    model=model,                     # すでにロード済みの AutoModel\n",
    "    tokenizer=tokenizer,             # すでにロード済みの Tokenizer\n",
    "    device=0                         # GPU を使うなら 0（CPU の場合は -1）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"validation\"][1][co]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
