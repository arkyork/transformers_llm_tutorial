{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelineの使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUが使える環境なら\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch] ipywidgets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# pipelineではモデルの指定をしなくても利用可能。\n",
    "# 最低限のコードで様々なモデルが利用可能。\n",
    "\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"When I lost my wallet\" のような文の続きを生成します。\n",
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "result = generator(\"When I lost my wallet,\", max_length=50, num_return_sequences=1)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられた文がポジティブかネガティブかを判定します。\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love using Transformers!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固有表現抽出\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章中の人名、地名、組織名などを抽出します。\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner(\"My name is Sarah and I live in London.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 長い文章を要約して短くします。\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "text = \"\"\"Transformers are models that process sequential input data by using attention mechanisms.\n",
    "They have become the model of choice for natural language processing tasks.\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=50, min_length=20)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問応答（Question Answering）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文脈を与えて、それに対する質問に答えさせる形式です。\n",
    "\n",
    "qa = pipeline(\"question-answering\")\n",
    "result = qa({\n",
    "    \"question\": \"Where do I live?\",\n",
    "    \"context\": \"My name is Sarah and I live in London.\"\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特定のモデルを読み込む方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近登場したQwen3の場合\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen3-0.6B\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen3-8B\n",
    "\n",
    "llama3 8bの場合は30GBくらいあった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートで動かすなら 0.6Bがおすすめ\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"おはよう！\"},\n",
    "]\n",
    "generator = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=generator(messages,max_length=250, num_return_sequences=1)\n",
    "\n",
    "print(text[0][\"generated_text\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを直接読み込んだ後にpipelineを作成する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# モデルとトークナイザーを先に読み込む\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 読み込んだモデル・トークナイザーから pipeline を作成\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは親切で賢い日本語アシスタントです。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"おはよう！\"}\n",
    "]\n",
    "\n",
    "\n",
    "result = generator(messages, max_new_tokens=250)\n",
    "print(result[0][\"generated_text\"][2][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmZemi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
